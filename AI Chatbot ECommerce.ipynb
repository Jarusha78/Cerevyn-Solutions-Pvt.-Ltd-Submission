{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Powered Customer Support Chatbot\n",
    "\n",
    "This project implements an intelligent conversational assistant for\n",
    "customer support using NLP techniques, web scraping, and a Flask-based API.\n",
    "\n",
    "Features:\n",
    "- Domain-specific question answering\n",
    "- Follow-up question handling\n",
    "- Intent detection\n",
    "- Product scraping and indexing\n",
    "- Real-time WebSocket communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# NLP imports\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# URL to Name\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_to_name(url):\n",
    "    parsed = urlparse(url)\n",
    "\n",
    "    # ---- Extract domain safely ----\n",
    "    netloc = parsed.netloc\n",
    "\n",
    "    # Remove common subdomains like www, m, shop, app, etc.\n",
    "    parts = netloc.split(\".\")\n",
    "    if parts[0] in [\"www\", \"m\", \"app\", \"shop\"]:\n",
    "        parts = parts[1:]  # remove prefix\n",
    "    \n",
    "    # domain name (before TLD)\n",
    "    domain = parts[0]\n",
    "\n",
    "    # ---- Extract path parts ----\n",
    "    path_parts = [p for p in parsed.path.split('/') if p]\n",
    "\n",
    "    # ---- Build final name ----\n",
    "    if path_parts:\n",
    "        return domain + \"_\" + \"_\".join(path_parts)\n",
    "    else:\n",
    "        return domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- USER CONFIG -----------------\n",
    "BASE_URL = \"https://gangslifestyle.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pavan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pavan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pavan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\pavan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "JSON_ENDPOINT = f\"{BASE_URL}/products.json\"\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)'\n",
    "}\n",
    "name = url_to_name(BASE_URL);\n",
    "OUTPUT_CSV = f\"{name}.csv\"    \n",
    "\n",
    "# Preprocessing config (from your Code 2)\n",
    "CATEGORY_COVERAGE_THRESHOLD = 0.5\n",
    "VARIANT_COLOR_COMBINE = True\n",
    "SUMMARY_SENTENCES = 1\n",
    "# ------------------------------------------------\n",
    "\n",
    "# Ensure resources for NLTK (will check & download if needed)\n",
    "nltk_data = ['punkt', 'stopwords', 'wordnet', 'omw-1.4']\n",
    "for r in nltk_data:\n",
    "    try:\n",
    "        nltk.data.find(r)\n",
    "    except Exception:\n",
    "        nltk.download(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- SCRAPER (variant-mode) -----------------\n",
    "def scrape_shopify_products(json_endpoint=JSON_ENDPOINT, headers=HEADERS, sleep_sec=1):\n",
    "    all_variants_data = []\n",
    "    page = 1\n",
    "    print(\"--- Starting Scrape (Shopify JSON - Detailed Variant Mode) ---\")\n",
    "    while True:\n",
    "        url = f\"{json_endpoint}?page={page}&limit=250\"\n",
    "        print(f\"Fetching Page {page}: {url}\")\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=20)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"  Status code {response.status_code}. Stopping.\")\n",
    "                break\n",
    "            data = response.json()\n",
    "            if 'products' not in data or not data['products']:\n",
    "                print(\"  No more products found in JSON. Crawl finished.\")\n",
    "                break\n",
    "\n",
    "            products = data['products']\n",
    "            print(f\"  Found {len(products)} products on page {page} ...\")\n",
    "\n",
    "            for product in products:\n",
    "                try:\n",
    "                    product_title = product.get('title', 'N/A')\n",
    "                    handle = product.get('handle', '')\n",
    "                    vendor = product.get('vendor', 'N/A')\n",
    "                    category = product.get('product_type', 'N/A')\n",
    "\n",
    "                    raw_html = product.get('body_html', '')\n",
    "                    if raw_html:\n",
    "                        soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "                        functional_details = soup.get_text(separator=' ', strip=True)\n",
    "                    else:\n",
    "                        functional_details = \"N/A\"\n",
    "\n",
    "                    # tags: product['tags'] may be a string or list depending on store — handle both\n",
    "                    tags_val = product.get('tags', '')\n",
    "                    if isinstance(tags_val, list):\n",
    "                        tags = ', '.join(tags_val)\n",
    "                    else:\n",
    "                        # sometimes Shopify returns a single string with commas\n",
    "                        tags = tags_val if tags_val else ''\n",
    "\n",
    "                    main_image_url = \"N/A\"\n",
    "                    if product.get('images'):\n",
    "                        # images is list of dicts with 'src'\n",
    "                        try:\n",
    "                            if isinstance(product['images'], list) and product['images']:\n",
    "                                main_image_url = product['images'][0].get('src', 'N/A')\n",
    "                            else:\n",
    "                                main_image_url = product['images']\n",
    "                        except Exception:\n",
    "                            main_image_url = \"N/A\"\n",
    "\n",
    "                    for variant in product.get('variants', []):\n",
    "                        variant_title = variant.get('title', 'N/A')\n",
    "                        variant_id = variant.get('id')\n",
    "                        price = variant.get('price', 'N/A')\n",
    "                        original_price = variant.get('compare_at_price')  # \"was\" price\n",
    "                        sku = variant.get('sku', 'N/A')\n",
    "                        available = variant.get('available', False)\n",
    "\n",
    "                        link = f\"{BASE_URL}/products/{handle}?variant={variant_id}\" if handle and variant_id else f\"{BASE_URL}\"\n",
    "\n",
    "                        if original_price and original_price != price:\n",
    "                            discount_info = f\"Was {original_price}\"\n",
    "                        else:\n",
    "                            discount_info = \"No Discount\"\n",
    "\n",
    "                        all_variants_data.append({\n",
    "                            'Product Name': product_title,\n",
    "                            'Variant Name': variant_title,\n",
    "                            'SKU': sku,\n",
    "                            'In Stock?': available,\n",
    "                            'Price': price,\n",
    "                            'Original Price': original_price if original_price else \"\",\n",
    "                            'Discount Info': discount_info,\n",
    "                            'Vendor (Brand)': vendor,\n",
    "                            'Category': category,\n",
    "                            'Tags': tags,\n",
    "                            'Functional Details': functional_details,\n",
    "                            'Link': link,\n",
    "                            'Main Image URL': main_image_url\n",
    "                        })\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error parsing product '{product.get('title', 'unknown')}': {e}\")\n",
    "\n",
    "            page += 1\n",
    "            time.sleep(sleep_sec)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error fetching URL or parsing JSON: {e}\")\n",
    "            break\n",
    "\n",
    "    return pd.DataFrame(all_variants_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- PREPROCESSING PIPELINE (adapted from Code 2) -----------------\n",
    "# Helpers (kept from your code)\n",
    "_currency_re = re.compile(r'[^\\d.,\\-]+')\n",
    "def parse_price(v):\n",
    "    if pd.isna(v):\n",
    "        return np.nan\n",
    "    s = str(v).strip()\n",
    "    if s == '' or s.lower() in ['nan','none','null']:\n",
    "        return np.nan\n",
    "    s = _currency_re.sub('', s).replace(',', '')\n",
    "    try:\n",
    "        return float(s)\n",
    "    except:\n",
    "        nums = re.findall(r'[-+]?\\d*\\.\\d+|\\d+', s)\n",
    "        return float(nums[0]) if nums else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_stock(v):\n",
    "    if pd.isna(v): return 'Unknown'\n",
    "    s = str(v).strip().lower()\n",
    "    if s in ['true','yes','1','in stock','available','instock', 'True', 'TRUE', '1']: return 'In Stock'\n",
    "    if s in ['false','no','0','out of stock','sold out','not available', 'False', 'FALSE', '0']: return 'Out of Stock'\n",
    "    return 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARKETING_WORDS = {'buy now','best','new','free shipping','hot','sale','discount','offer','trending'}\n",
    "def clean_title(t):\n",
    "    if pd.isna(t): return ''\n",
    "    s = re.sub(r'\\s+', ' ', str(t).strip())\n",
    "    for w in MARKETING_WORDS:\n",
    "        # case-insensitive whole-word removal\n",
    "        s = re.sub(r'(?i)\\b' + re.escape(w) + r'\\b', '', s)\n",
    "    return re.sub(r'\\s+', ' ', s).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOR_WORDS = {'black','white','red','blue','green','yellow','pink','orange','purple','brown','grey','gray','silver','gold','navy'}\n",
    "def variant_looks_like_color(v):\n",
    "    if pd.isna(v) or str(v).strip() == '': return False\n",
    "    parts = re.split(r'[,/;|-]+', str(v).lower())\n",
    "    return any(p.strip() in COLOR_WORDS for p in parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_functional_text(txt):\n",
    "    if pd.isna(txt): return ''\n",
    "    s = re.sub(r'<[^>]+>', ' ', str(txt))\n",
    "    s = re.sub(r'[\\r\\n\\t]+', ' ', s)\n",
    "    return re.sub(r'\\s+', ' ', s).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    # safe guard: if text empty\n",
    "    if not isinstance(text, str) or text.strip() == '':\n",
    "        return ''\n",
    "    words = word_tokenize(text)\n",
    "    return ' '.join(lemmatizer.lemmatize(w.lower()) for w in words if w.isalnum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractive_summary(text, n_sentences=1):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return ''\n",
    "    sents = sent_tokenize(text)\n",
    "    if len(sents) <= n_sentences:\n",
    "        return ' '.join(sents)\n",
    "    try:\n",
    "        vec = TfidfVectorizer(stop_words='english')\n",
    "        X = vec.fit_transform(sents)\n",
    "        centroid = X.sum(axis=0)\n",
    "        scores = X.dot(centroid.T).A.ravel()\n",
    "        idx = scores.argsort()[::-1][:n_sentences]\n",
    "        idx = sorted(idx)\n",
    "        return ' '.join(sents[i].strip() for i in idx)\n",
    "    except Exception:\n",
    "        return sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_url(u):\n",
    "    if pd.isna(u): return ''\n",
    "    s = str(u).strip()\n",
    "    return re.sub(r'[\\?&]$', '', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def looks_like_image_url(u):\n",
    "    if pd.isna(u): return False\n",
    "    return bool(re.search(r'\\.(jpg|jpeg|png|webp|gif)$', str(u), flags=re.I))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_variants_df(df_raw):\n",
    "    \"\"\"\n",
    "    Accepts raw DataFrame similar to Code 1 output and returns cleaned DataFrame similar to Code 2 output.\n",
    "    \"\"\"\n",
    "    if df_raw is None or df_raw.shape[0] == 0:\n",
    "        print(\"No raw data to preprocess.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    # Standardize column names if needed (allow both exact names and a few variants)\n",
    "    # Map expected input columns to short keys used inside pipeline.\n",
    "    col_map = {\n",
    "        'Product Name': 'title',\n",
    "        'Variant Name': 'variant',\n",
    "        'SKU': 'sku',\n",
    "        'In Stock?': 'instock',\n",
    "        'Price': 'price',\n",
    "        'Original Price': 'original_price',\n",
    "        'Discount Info': 'discount_info',\n",
    "        'Category': 'category',\n",
    "        'Tags': 'tags',\n",
    "        'Functional Details': 'functional',\n",
    "        'Link': 'product_url',\n",
    "        'Main Image URL': 'image_url'\n",
    "    }\n",
    "\n",
    "    # Create working frame with keys from col_map\n",
    "    working = pd.DataFrame()\n",
    "    for short, long in col_map.items():\n",
    "        if short in df.columns:\n",
    "            working[long] = df[short].fillna('')\n",
    "        else:\n",
    "            # if expected column not present, create empty series\n",
    "            working[long] = ''\n",
    "\n",
    "    # 1. Clean title\n",
    "    working['title'] = working['title'].apply(clean_title)\n",
    "\n",
    "    # 2. Combine variant into title when variant looks like a color\n",
    "    if VARIANT_COLOR_COMBINE:\n",
    "        # ensure we have 'variant' present\n",
    "        working['variant'] = working.get('variant', '')\n",
    "        working['title'] = working.apply(\n",
    "            lambda r: f\"{r['title']} (Color: {r['variant']})\" if variant_looks_like_color(r['variant']) else r['title'],\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    # 3. SKU fallback\n",
    "    working['sku'] = working['sku'].astype(str).str.strip()\n",
    "    missing = working['sku'] == ''\n",
    "    if missing.any():\n",
    "        working.loc[missing, 'sku'] = [f\"MISSINGSKU_{i}\" for i in range(1, missing.sum() + 1)]\n",
    "\n",
    "    # 4. Stock normalize\n",
    "    working['stock_status'] = working['instock'].apply(normalize_stock)\n",
    "\n",
    "    # 5. Price parsing\n",
    "    working['price_parsed'] = working['price'].apply(parse_price)\n",
    "    working['original_price_parsed'] = working['original_price'].apply(parse_price)\n",
    "\n",
    "    def compute_prices(r):\n",
    "        p = r['price_parsed']\n",
    "        o = r['original_price_parsed']\n",
    "        if pd.isna(o) or o == 0:\n",
    "            o = p\n",
    "        if pd.isna(p) and not pd.isna(o):\n",
    "            p = o\n",
    "        if pd.isna(p) or pd.isna(o):\n",
    "            disc = np.nan\n",
    "        else:\n",
    "            disc = 0 if o == p else round((o - p) / o * 100, 1) if o > p else 0\n",
    "        return pd.Series([p, o, disc])\n",
    "\n",
    "    working[['price_current', 'price_original', 'discount_percent']] = working.apply(compute_prices, axis=1)\n",
    "\n",
    "    # 6. Category/tags remove if < threshold\n",
    "    def keep(col):\n",
    "        filled = (working[col].astype(str).str.strip() != '').sum()\n",
    "        return (filled / len(working)) >= CATEGORY_COVERAGE_THRESHOLD\n",
    "\n",
    "    if not keep('category'):\n",
    "        if 'category' in working.columns:\n",
    "            working.drop(columns=['category'], inplace=True)\n",
    "\n",
    "    if not keep('tags'):\n",
    "        if 'tags' in working.columns:\n",
    "            working.drop(columns=['tags'], inplace=True)\n",
    "\n",
    "    # 7. Clean functional description\n",
    "    working['long_description'] = working['functional'].apply(clean_functional_text)\n",
    "\n",
    "    # 8. Summary\n",
    "    working['summary'] = working['long_description'].apply(lambda t: extractive_summary(t, SUMMARY_SENTENCES))\n",
    "\n",
    "    # 9. Lemmatized text (safe-guard for heavy computation)\n",
    "    working['indexed_text_lemma'] = working.apply(\n",
    "        lambda r: lemmatize_text(str(r.get('title','')) + ' ' + str(r.get('summary','')) + ' ' + str(r.get('long_description',''))),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # 10. Clean URL\n",
    "    working['product_url'] = working['product_url'].apply(clean_url)\n",
    "\n",
    "    # 11. Image URL rule (keep or drop)\n",
    "    img_frac = working['image_url'].apply(looks_like_image_url).mean()\n",
    "    if 'image_url' in working.columns and img_frac > 0.99:\n",
    "        working.drop(columns=['image_url'], inplace=True)\n",
    "\n",
    "    # 12. Final search content\n",
    "    working['search_content'] = (\n",
    "        working.get('title', '') + \" \" + working.get('summary', '') + \" \" + working.get('long_description', '')\n",
    "    )\n",
    "\n",
    "    # Final export columns (NO vendor, NO notes)\n",
    "    export_cols = [\n",
    "        'sku','title','price_current','price_original','discount_percent',\n",
    "        'stock_status','summary','long_description','search_content',\n",
    "        'indexed_text_lemma','product_url'\n",
    "    ]\n",
    "    export_cols = [c for c in export_cols if c in working.columns]\n",
    "    cleaned = working[export_cols].copy()\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- Main flow -----------------\n",
    "def main():\n",
    "    # 1) Scrape\n",
    "    raw_df = scrape_shopify_products()\n",
    "\n",
    "    if raw_df is None or raw_df.shape[0] == 0:\n",
    "        print(\"No data scraped. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Save raw CSV (optional)\n",
    "    try:\n",
    "        raw_df.to_csv(OUTPUT_CSV, index=False, encoding='utf-8')\n",
    "        print(\"Saved raw variants CSV:\", OUTPUT_CSV)\n",
    "    except Exception as e:\n",
    "        print(\"Could not save raw CSV:\", e)\n",
    "\n",
    "    # 2) Preprocess\n",
    "    cleaned = preprocess_variants_df(raw_df)\n",
    "\n",
    "    # 3) Save cleaned CSV\n",
    "    try:\n",
    "        cleaned.to_csv(OUTPUT_CSV, index=False, encoding='utf-8')\n",
    "        print(\"Saved cleaned CSV:\", OUTPUT_CSV)\n",
    "        print(\"Rows in cleaned:\", len(cleaned))\n",
    "    except Exception as e:\n",
    "        print(\"Could not save cleaned CSV:\", e)\n",
    "\n",
    "    # show head for quick check\n",
    "    print(\"\\n--- Raw head ---\")\n",
    "    print(raw_df.head())\n",
    "    print(\"\\n--- Cleaned head ---\")\n",
    "    print(cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T15:26:42.177608Z",
     "iopub.status.busy": "2025-11-24T15:26:42.177406Z",
     "iopub.status.idle": "2025-11-24T15:26:43.907550Z",
     "shell.execute_reply": "2025-11-24T15:26:43.906708Z",
     "shell.execute_reply.started": "2025-11-24T15:26:42.177592Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Scrape (Shopify JSON - Detailed Variant Mode) ---\n",
      "Fetching Page 1: https://gangslifestyle.com//products.json?page=1&limit=250\n",
      "  Found 32 products on page 1 ...\n",
      "Fetching Page 2: https://gangslifestyle.com//products.json?page=2&limit=250\n",
      "  No more products found in JSON. Crawl finished.\n",
      "Saved raw variants CSV: gangslifestyle.csv\n",
      "Saved cleaned CSV: gangslifestyle.csv\n",
      "Rows in cleaned: 282\n",
      "\n",
      "--- Raw head ---\n",
      "                Product Name                Variant Name           SKU  \\\n",
      "0  The VelocityTrunk Luggage           Dark Grey / Small   GANGS2036-S   \n",
      "1  The VelocityTrunk Luggage          Dark Grey / Medium   GANGS2036-M   \n",
      "2  The VelocityTrunk Luggage           Dark Grey / Large   GANGS2036-L   \n",
      "3  The VelocityTrunk Luggage  Dark Grey / Small + Medium  GANGS2036-SM   \n",
      "4  The VelocityTrunk Luggage   Dark Grey / Small + Large  GANGS2036-SL   \n",
      "\n",
      "   In Stock?    Price Original Price Discount Info   Vendor (Brand)  \\\n",
      "0       True  4599.00       37999.00  Was 37999.00  GANGS Lifestyle   \n",
      "1       True  5299.00       37999.00  Was 37999.00  GANGS Lifestyle   \n",
      "2       True  6499.00       37999.00  Was 37999.00  GANGS Lifestyle   \n",
      "3       True  8599.00       37999.00  Was 37999.00  GANGS Lifestyle   \n",
      "4       True  8999.00       37999.00  Was 37999.00  GANGS Lifestyle   \n",
      "\n",
      "       Category                                               Tags  \\\n",
      "0  luggage bags  achromatic exquisite 25L backpack, Adventure B...   \n",
      "1  luggage bags  achromatic exquisite 25L backpack, Adventure B...   \n",
      "2  luggage bags  achromatic exquisite 25L backpack, Adventure B...   \n",
      "3  luggage bags  achromatic exquisite 25L backpack, Adventure B...   \n",
      "4  luggage bags  achromatic exquisite 25L backpack, Adventure B...   \n",
      "\n",
      "                                  Functional Details  \\\n",
      "0  Introducing the epitome of style and functiona...   \n",
      "1  Introducing the epitome of style and functiona...   \n",
      "2  Introducing the epitome of style and functiona...   \n",
      "3  Introducing the epitome of style and functiona...   \n",
      "4  Introducing the epitome of style and functiona...   \n",
      "\n",
      "                                                Link  \\\n",
      "0  https://gangslifestyle.com//products/the-veloc...   \n",
      "1  https://gangslifestyle.com//products/the-veloc...   \n",
      "2  https://gangslifestyle.com//products/the-veloc...   \n",
      "3  https://gangslifestyle.com//products/the-veloc...   \n",
      "4  https://gangslifestyle.com//products/the-veloc...   \n",
      "\n",
      "                                      Main Image URL  \n",
      "0  https://cdn.shopify.com/s/files/1/0797/9778/07...  \n",
      "1  https://cdn.shopify.com/s/files/1/0797/9778/07...  \n",
      "2  https://cdn.shopify.com/s/files/1/0797/9778/07...  \n",
      "3  https://cdn.shopify.com/s/files/1/0797/9778/07...  \n",
      "4  https://cdn.shopify.com/s/files/1/0797/9778/07...  \n",
      "\n",
      "--- Cleaned head ---\n",
      "            sku                      title  price_current  price_original  \\\n",
      "0   GANGS2036-S  The VelocityTrunk Luggage         4599.0         37999.0   \n",
      "1   GANGS2036-M  The VelocityTrunk Luggage         5299.0         37999.0   \n",
      "2   GANGS2036-L  The VelocityTrunk Luggage         6499.0         37999.0   \n",
      "3  GANGS2036-SM  The VelocityTrunk Luggage         8599.0         37999.0   \n",
      "4  GANGS2036-SL  The VelocityTrunk Luggage         8999.0         37999.0   \n",
      "\n",
      "   discount_percent stock_status  \\\n",
      "0              87.9     In Stock   \n",
      "1              86.1     In Stock   \n",
      "2              82.9     In Stock   \n",
      "3              77.4     In Stock   \n",
      "4              76.3     In Stock   \n",
      "\n",
      "                                             summary  \\\n",
      "0  Crafted with meticulous attention to detail, t...   \n",
      "1  Crafted with meticulous attention to detail, t...   \n",
      "2  Crafted with meticulous attention to detail, t...   \n",
      "3  Crafted with meticulous attention to detail, t...   \n",
      "4  Crafted with meticulous attention to detail, t...   \n",
      "\n",
      "                                    long_description  \\\n",
      "0  Introducing the epitome of style and functiona...   \n",
      "1  Introducing the epitome of style and functiona...   \n",
      "2  Introducing the epitome of style and functiona...   \n",
      "3  Introducing the epitome of style and functiona...   \n",
      "4  Introducing the epitome of style and functiona...   \n",
      "\n",
      "                                      search_content  \\\n",
      "0  The VelocityTrunk Luggage Crafted with meticul...   \n",
      "1  The VelocityTrunk Luggage Crafted with meticul...   \n",
      "2  The VelocityTrunk Luggage Crafted with meticul...   \n",
      "3  The VelocityTrunk Luggage Crafted with meticul...   \n",
      "4  The VelocityTrunk Luggage Crafted with meticul...   \n",
      "\n",
      "                                  indexed_text_lemma  \\\n",
      "0  the velocitytrunk luggage crafted with meticul...   \n",
      "1  the velocitytrunk luggage crafted with meticul...   \n",
      "2  the velocitytrunk luggage crafted with meticul...   \n",
      "3  the velocitytrunk luggage crafted with meticul...   \n",
      "4  the velocitytrunk luggage crafted with meticul...   \n",
      "\n",
      "                                         product_url  \n",
      "0  https://gangslifestyle.com//products/the-veloc...  \n",
      "1  https://gangslifestyle.com//products/the-veloc...  \n",
      "2  https://gangslifestyle.com//products/the-veloc...  \n",
      "3  https://gangslifestyle.com//products/the-veloc...  \n",
      "4  https://gangslifestyle.com//products/the-veloc...  \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GangsLifeStyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Optional NLTK for sentence splitting\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimal nltk assets required (download if missing)\n",
    "for r in ['punkt']:\n",
    "    try:\n",
    "        nltk.data.find(r)\n",
    "    except Exception:\n",
    "        try:\n",
    "            nltk.download(r, quiet=True)\n",
    "        except Exception:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- CONFIG ----------\n",
    "CSV_PATH = \"gangslifestyle.csv\"   # <- change if needed\n",
    "SIMILARITY_THRESHOLD = 0.30\n",
    "TOP_K_DEFAULT = 3\n",
    "OUTPUT_DIR = \"/kaggle/working\"\n",
    "\n",
    "# ---------- HELPERS ----------\n",
    "IRRELEVANT_KEYWORDS = [\n",
    "    \"gym\",\"dumbbell\",\"exercise\",\"workout\",\"recipe\",\"food\",\"cooking\",\n",
    "    \"weather\",\"news\",\"politics\",\"relationship\",\"doctor\",\"medicine\",\n",
    "    \"math\",\"code\",\"python\",\"java\",\"cpp\"\n",
    "]\n",
    "_irrelevant_regex = re.compile(r'\\b(' + r'|'.join(re.escape(w) for w in IRRELEVANT_KEYWORDS) + r')\\b', flags=re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_irrelevant(query: str) -> bool:\n",
    "    if not query or str(query).strip() == \"\":\n",
    "        return True\n",
    "    return bool(_irrelevant_regex.search(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_jsonify(obj):\n",
    "    \"\"\"Convert numpy / pandas types to JSON-serializable types.\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {safe_jsonify(k): safe_jsonify(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, list):\n",
    "        return [safe_jsonify(x) for x in obj]\n",
    "    if isinstance(obj, (np.integer,)):\n",
    "        return int(obj)\n",
    "    if isinstance(obj, (np.floating,)):\n",
    "        return float(obj)\n",
    "    if isinstance(obj, (np.ndarray,)):\n",
    "        return obj.tolist()\n",
    "    if pd.isna(obj):\n",
    "        return None\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_text_field(df: pd.DataFrame) -> str:\n",
    "    \"\"\"Pick the best field to index for retrieval (prefer indexed_text_lemma, then search_content, else fallback).\"\"\"\n",
    "    if 'indexed_text_lemma' in df.columns and df['indexed_text_lemma'].astype(str).str.strip().any():\n",
    "        return 'indexed_text_lemma'\n",
    "    if 'search_content' in df.columns and df['search_content'].astype(str).str.strip().any():\n",
    "        return 'search_content'\n",
    "    for candidate in ['long_description','description','title']:\n",
    "        if candidate in df.columns:\n",
    "            return candidate\n",
    "    return df.columns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tfidf_index(df: pd.DataFrame, field: str):\n",
    "    vec = TfidfVectorizer(ngram_range=(1,2), min_df=1)\n",
    "    mat = vec.fit_transform(df[field].astype(str).fillna('').values)\n",
    "    return vec, mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_search_from_index(query: str,\n",
    "                             df: pd.DataFrame,\n",
    "                             vectorizer: TfidfVectorizer,\n",
    "                             tfidf_matrix,\n",
    "                             text_field: str,\n",
    "                             top_k: int):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with:\n",
    "      clean_text, product_title, url, score, sku,\n",
    "      price_current, price_original, discount_percent, stock_status,\n",
    "      summary, long_description\n",
    "    \"\"\"\n",
    "    q_vec = vectorizer.transform([query])\n",
    "    sims = cosine_similarity(q_vec, tfidf_matrix).ravel()\n",
    "    idxs = sims.argsort()[::-1][:top_k]\n",
    "    rows = []\n",
    "    for i in idxs:\n",
    "        row = df.iloc[i]\n",
    "        rows.append({\n",
    "            'clean_text': row.get('search_content', '') or row.get(text_field, ''),\n",
    "            'product_title': row.get('title',''),\n",
    "            'url': row.get('product_url','') or row.get('url',''),\n",
    "            'score': float(sims[i]),\n",
    "            'sku': row.get('sku',''),\n",
    "            'price_current': row.get('price_current',''),\n",
    "            'price_original': row.get('price_original',''),\n",
    "            'discount_percent': row.get('discount_percent',''),\n",
    "            'stock_status': row.get('stock_status',''),\n",
    "            'summary': row.get('summary',''),\n",
    "            'long_description': row.get('long_description','')\n",
    "        })\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractive_summary_from_retrieved(retrieved_df, top_k=TOP_K_DEFAULT):\n",
    "    if retrieved_df is None or retrieved_df.empty:\n",
    "        return \"\"\n",
    "    merged = \" \".join(str(x) for x in retrieved_df['clean_text'].astype(str).head(top_k).tolist())\n",
    "    merged = re.sub(r'[^a-zA-Z0-9\\s\\.\\,\\-]', ' ', merged)\n",
    "    # split into sentences\n",
    "    sents = re.split(r'(?<=[.!?])\\s+', merged)\n",
    "    return \" \".join(sents[:2]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- FOLLOW-UP DETECTION ----------\n",
    "def looks_like_followup(query: str, state: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Decide if this question should use the last product context.\n",
    "    We want single-word follow-ups like \"Price\", \"Discount\", \"Stock\", \"Status\"\n",
    "    to automatically refer to the last product.\n",
    "    \"\"\"\n",
    "    q = query.strip().lower()\n",
    "    if not state.get(\"last_top_product_title\"):\n",
    "        return False\n",
    "\n",
    "    # obvious conversational starters\n",
    "    if q.startswith(\"then \") or q.startswith(\"what about\") or q.startswith(\"and \"):\n",
    "        return True\n",
    "\n",
    "    # generic follow-up keywords (NOW includes 'discount' and 'offer')\n",
    "    follow_keywords = [\n",
    "        \"price\", \"cost\", \"rate\",\n",
    "        \"discount\", \"offer\",\n",
    "        \"color\", \"colour\", \"size\",\n",
    "        \"details\", \"more about\", \"explain\",\n",
    "        \"stock\", \"availability\", \"status\"\n",
    "    ]\n",
    "    if any(kw in q for kw in follow_keywords) and len(q.split()) <= 7:\n",
    "        return True\n",
    "\n",
    "    # references to previous / above product\n",
    "    if (\"above\" in q or \"previous\" in q or \"earlier\" in q) and any(\n",
    "        w in q for w in [\"product\", \"item\", \"one\"]\n",
    "    ):\n",
    "        return True\n",
    "\n",
    "    # vague references, still treat as follow-up if short\n",
    "    if any(w in q for w in [\"this\", \"that\", \"it\", \"above\", \"previous\", \"earlier\"]) and len(q.split()) <= 10:\n",
    "        return True\n",
    "\n",
    "    # extra safety: if query is VERY short (1–2 words) and looks like a price/stock/discount intent, use follow-up\n",
    "    if len(q.split()) <= 2 and detect_question_intent(query) in [\"price\", \"discount\", \"stock\", \"orig_price\"]:\n",
    "        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- INTENT DETECTION ----------\n",
    "def detect_question_intent(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns one of: 'discount', 'orig_price', 'price', 'stock', 'detail', 'general'\n",
    "    \"\"\"\n",
    "    q = query.lower()\n",
    "    # discount / offer\n",
    "    if any(kw in q for kw in [\"discount\", \"offer\", \"% off\", \"off \"]):\n",
    "        return \"discount\"\n",
    "    # original price / mrp\n",
    "    if any(kw in q for kw in [\"original price\", \"mrp\", \"actual price\", \"before discount\"]):\n",
    "        return \"orig_price\"\n",
    "    # current price / price / cost\n",
    "    if any(kw in q for kw in [\"price\", \"cost\", \"rate\"]):\n",
    "        return \"price\"\n",
    "    # stock / availability / status\n",
    "    if any(kw in q for kw in [\"stock\", \"in stock\", \"out of stock\", \"available\", \"availability\", \"status\"]):\n",
    "        return \"stock\"\n",
    "    # more details / explanation (NOTE: we only trigger on *more* / explain style, not just the word \"details\")\n",
    "    detail_phrases = [\n",
    "        \"more detail\", \"more details\", \"more in detail\",\n",
    "        \"tell me more\", \"explain\", \"explanation\",\n",
    "        \"full description\", \"describe\", \"more info\", \"more about this\"\n",
    "    ]\n",
    "    if any(p in q for p in detail_phrases):\n",
    "        return \"detail\"\n",
    "    return \"general\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- MAIN HANDLER WITH CONTEXT ----------\n",
    "def handle_query_with_context(df: pd.DataFrame,\n",
    "                              query: str,\n",
    "                              vectorizer: TfidfVectorizer,\n",
    "                              tfidf_matrix,\n",
    "                              text_field: str,\n",
    "                              state: dict,\n",
    "                              top_k: int = TOP_K_DEFAULT):\n",
    "    \"\"\"\n",
    "    df: dataframe loaded from CSV\n",
    "    query: user question\n",
    "    vectorizer, tfidf_matrix, text_field: pre-built retrieval objects\n",
    "    state: dict with conversation memory (mutated in-place)\n",
    "    \"\"\"\n",
    "    fallback_msg = \"Sorry, I couldn't answer that. I can assist you with product, website, business, or item-related queries.\"\n",
    "\n",
    "    if is_irrelevant(query):\n",
    "        result = {\"query\": query, \"top_results\": [], \"final_answer\": fallback_msg, \"product_links\": []}\n",
    "        state[\"last_query\"] = query\n",
    "        state[\"last_result\"] = None\n",
    "        state[\"last_top_product_title\"] = None\n",
    "        return result, state\n",
    "\n",
    "    # Decide whether to use previous context\n",
    "    use_followup = looks_like_followup(query, state)\n",
    "\n",
    "    if use_followup:\n",
    "        base_title = state.get(\"last_top_product_title\", \"\")\n",
    "        expanded_query = f\"{query.strip()} for product: {base_title}\"\n",
    "        search_query = expanded_query\n",
    "    else:\n",
    "        search_query = query\n",
    "\n",
    "    # Retrieval\n",
    "    retrieved = query_search_from_index(query=search_query,\n",
    "                                        df=df,\n",
    "                                        vectorizer=vectorizer,\n",
    "                                        tfidf_matrix=tfidf_matrix,\n",
    "                                        text_field=text_field,\n",
    "                                        top_k=top_k)\n",
    "    if retrieved.empty:\n",
    "        result = {\"query\": query, \"top_results\": [], \"final_answer\": fallback_msg, \"product_links\": []}\n",
    "        state[\"last_query\"] = query\n",
    "        state[\"last_result\"] = retrieved\n",
    "        state[\"last_top_product_title\"] = None\n",
    "        return result, state\n",
    "\n",
    "    best_score = float(retrieved['score'].max())\n",
    "    if best_score < SIMILARITY_THRESHOLD:\n",
    "        result = {\n",
    "            \"query\": query,\n",
    "            \"top_results\": safe_jsonify(retrieved.to_dict(orient='records')),\n",
    "            \"final_answer\": fallback_msg,\n",
    "            \"product_links\": []\n",
    "        }\n",
    "        state[\"last_query\"] = query\n",
    "        state[\"last_result\"] = retrieved\n",
    "        state[\"last_top_product_title\"] = None\n",
    "        return result, state\n",
    "\n",
    "    # ---------- SPECIAL ANSWERS ----------\n",
    "    intent = detect_question_intent(query)\n",
    "    top = retrieved.iloc[0]\n",
    "    title = (top.get(\"product_title\") or \"\").strip()\n",
    "\n",
    "    price_current = str(top.get(\"price_current\") or \"\").strip()\n",
    "    price_original = str(top.get(\"price_original\") or \"\").strip()\n",
    "    discount_percent = str(top.get(\"discount_percent\") or \"\").strip()\n",
    "    stock_status = str(top.get(\"stock_status\") or \"\").strip()\n",
    "    summary_txt = str(top.get(\"summary\") or \"\").strip()\n",
    "    long_desc = str(top.get(\"long_description\") or \"\").strip()\n",
    "\n",
    "    final_answer = None\n",
    "\n",
    "    # 1) Price-type questions\n",
    "    if intent == \"price\":\n",
    "        if price_current and price_current.lower() != \"nan\":\n",
    "            final_answer = f\"The current price of {title} is {price_current}.\"\n",
    "            if discount_percent and discount_percent.lower() != \"nan\":\n",
    "                final_answer += f\" It currently has a discount of {discount_percent}%.\"\n",
    "        elif price_original and price_original.lower() != \"nan\":\n",
    "            final_answer = f\"The price information of {title} is not fully available, but the original price is {price_original}.\"\n",
    "    elif intent == \"orig_price\":\n",
    "        if price_original and price_original.lower() != \"nan\":\n",
    "            final_answer = f\"The original price (before discount) of {title} is {price_original}.\"\n",
    "        elif price_current and price_current.lower() != \"nan\":\n",
    "            final_answer = f\"The original price is not available, but the current price of {title} is {price_current}.\"\n",
    "    elif intent == \"discount\":\n",
    "        if discount_percent and discount_percent.lower() != \"nan\":\n",
    "            final_answer = f\"{title} currently has a discount of {discount_percent}%.\"\n",
    "        elif price_original and price_current and price_original.lower() != \"nan\" and price_current.lower() != \"nan\":\n",
    "            try:\n",
    "                po = float(price_original)\n",
    "                pc = float(price_current)\n",
    "                if po > 0:\n",
    "                    disc = round((po - pc) / po * 100, 1)\n",
    "                    final_answer = f\"{title} has an approximate discount of {disc}%.\"\n",
    "            except Exception:\n",
    "                pass\n",
    "    elif intent == \"stock\":\n",
    "        if stock_status and stock_status.lower() != \"nan\":\n",
    "            final_answer = f\"{title} is currently {stock_status}.\"\n",
    "        else:\n",
    "            final_answer = f\"The stock status of {title} is not clearly available.\"\n",
    "    # 2) Detail questions → use long_description\n",
    "    elif intent == \"detail\":\n",
    "        if long_desc and long_desc.lower() != \"nan\":\n",
    "            final_answer = f\"Here are more details about {title}: {long_desc}\"\n",
    "        elif summary_txt:\n",
    "            final_answer = f\"Here is a summary of {title}: {summary_txt}\"\n",
    "    # 3) General questions → use summary first\n",
    "    if intent == \"general\" and not final_answer:\n",
    "        if summary_txt and summary_txt.lower() != \"nan\":\n",
    "            final_answer = f\"{title}: {summary_txt}\"\n",
    "        else:\n",
    "            # fallback to extractive summary\n",
    "            final_answer = extractive_summary_from_retrieved(retrieved, top_k=top_k) or fallback_msg\n",
    "\n",
    "    # Safety: if none of the above set final_answer, still fallback\n",
    "    if not final_answer:\n",
    "        if summary_txt:\n",
    "            final_answer = f\"{title}: {summary_txt}\"\n",
    "        else:\n",
    "            final_answer = extractive_summary_from_retrieved(retrieved, top_k=top_k) or fallback_msg\n",
    "\n",
    "    product_links = [r.get('url','') for _, r in retrieved.head(top_k).iterrows() if r.get('url')]\n",
    "\n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"top_results\": safe_jsonify(retrieved.to_dict(orient='records')),\n",
    "        \"final_answer\": final_answer,\n",
    "        \"product_links\": product_links\n",
    "    }\n",
    "\n",
    "    # Update conversation state\n",
    "    state[\"last_query\"] = query\n",
    "    state[\"last_result\"] = retrieved\n",
    "    state[\"last_top_product_title\"] = top.get(\"product_title\") or \"\"\n",
    "\n",
    "    return result, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result_as_json(result: dict, base_name: str = \"query_response\"):\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    ts = int(time.time())\n",
    "    path = os.path.join(OUTPUT_DIR, f\"{base_name}_{ts}.json\")\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(safe_jsonify(result), f, indent=2, ensure_ascii=False)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- LOAD CSV + BUILD INDEX ONCE ----------\n",
    "if not os.path.exists(CSV_PATH):\n",
    "    raise FileNotFoundError(f\"CSV not found at: {CSV_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CSV with 282 rows from gangslifestyle.csv\n",
      "Indexing text field: indexed_text_lemma\n",
      "TF-IDF matrix shape: (282, 364)\n",
      "\n",
      "Type your questions. Type 'exit' or 'quit' to stop.\n",
      "\n",
      "\n",
      "Bot: Maximus Overnight Backpack - 30L: Introducing the epitome of style and functionality – the Premium Gang's Backpack.\n",
      "Links:\n",
      " - https://gangslifestyle.com//products/maximus-overnight-backpack-30l?variant=49701979586872\n",
      " - https://gangslifestyle.com//products/maximus-overnight-backpack-30l?variant=49701979554104\n",
      " - https://gangslifestyle.com//products/maximus-overnight-backpack-30l?variant=49701979521336\n",
      "(Saved response JSON to: /kaggle/working\\query_response_1770403553.json)\n",
      "\n",
      "\n",
      "Bot: The current price of Maximus Overnight Backpack - 30L is 1399.0. It currently has a discount of 53.4%.\n",
      "Links:\n",
      " - https://gangslifestyle.com//products/maximus-overnight-backpack-30l?variant=49701979586872\n",
      " - https://gangslifestyle.com//products/maximus-overnight-backpack-30l?variant=49701979554104\n",
      " - https://gangslifestyle.com//products/maximus-overnight-backpack-30l?variant=49701979521336\n",
      "(Saved response JSON to: /kaggle/working\\query_response_1770403559.json)\n",
      "\n",
      "\n",
      "Bot: Maximus Overnight Backpack - 30L currently has a discount of 53.4%.\n",
      "Links:\n",
      " - https://gangslifestyle.com//products/maximus-overnight-backpack-30l?variant=49701979586872\n",
      " - https://gangslifestyle.com//products/maximus-overnight-backpack-30l?variant=49701979554104\n",
      " - https://gangslifestyle.com//products/maximus-overnight-backpack-30l?variant=49701979521336\n",
      "(Saved response JSON to: /kaggle/working\\query_response_1770403564.json)\n",
      "\n",
      "\n",
      "Bot: Achromatic Exquisite Backpack - 25L: Introducing the epitome of style and functionality – the Premium Gang's Backpack.\n",
      "Links:\n",
      " - https://gangslifestyle.com//products/achromatic-exquisite-backpack?variant=49187685466424\n",
      " - https://gangslifestyle.com//products/maximus-overnight-backpack-30l?variant=49701979586872\n",
      " - https://gangslifestyle.com//products/maximus-overnight-backpack-30l?variant=49701979554104\n",
      "(Saved response JSON to: /kaggle/working\\query_response_1770403580.json)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH, dtype=str).fillna('')\n",
    "print(f\"Loaded CSV with {len(df)} rows from {CSV_PATH}\")\n",
    "\n",
    "text_field = choose_text_field(df)\n",
    "print(\"Indexing text field:\", text_field)\n",
    "vectorizer, tfidf_matrix = build_tfidf_index(df, text_field)\n",
    "print(\"TF-IDF matrix shape:\", tfidf_matrix.shape)\n",
    "\n",
    "# ---------- INTERACTIVE LOOP WITH CONTEXT ----------\n",
    "conversation_state = {\n",
    "    \"last_query\": None,\n",
    "    \"last_result\": None,\n",
    "    \"last_top_product_title\": None\n",
    "}\n",
    "\n",
    "print(\"\\nType your questions. Type 'exit' or 'quit' to stop.\\n\")\n",
    "\n",
    "while True:\n",
    "    user_query = input(\"You: \").strip()\n",
    "    if user_query.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Exiting.\")\n",
    "        break\n",
    "\n",
    "    result, conversation_state = handle_query_with_context(\n",
    "        df=df,\n",
    "        query=user_query,\n",
    "        vectorizer=vectorizer,\n",
    "        tfidf_matrix=tfidf_matrix,\n",
    "        text_field=text_field,\n",
    "        state=conversation_state,\n",
    "        top_k=TOP_K_DEFAULT\n",
    "    )\n",
    "\n",
    "    print(\"\\nBot:\", result[\"final_answer\"])\n",
    "    if result[\"product_links\"]:\n",
    "        print(\"Links:\")\n",
    "        for link in result[\"product_links\"]:\n",
    "            print(\" -\", link)\n",
    "\n",
    "    save_path = save_result_as_json(result, base_name=\"query_response\")\n",
    "    print(f\"(Saved response JSON to: {save_path})\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8737640,
     "sourceId": 13733027,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
